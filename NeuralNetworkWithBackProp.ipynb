{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "acfff295-704f-46e0-8304-b42ccf76b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def activation(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def derivative_activation(z):\n",
    "    return 1 - np.tanh(z)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67dda7e2-153a-42e5-8fca-55ef9d35deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, activation, derivative_activation, alpha, num):\n",
    "        self.W = np.random.random(num) # num is the number of neurons in previous layer or input vector size\n",
    "        self.b = np.random.random()\n",
    "        self.alpha = alpha # learning rate\n",
    "        self.loss = 0\n",
    "        self.X = []\n",
    "        self.grad = 0\n",
    "        self.activation = activation \n",
    "        self.derivative_activation = derivative_activation \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Weights={self.W} Bias={self.b} loss={self.loss}'\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X # X is a vector of size num\n",
    "        self.loss = 0\n",
    "        z = np.dot(self.W,self.X) + self.b \n",
    "        self.grad = self.derivative_activation(z)\n",
    "        return self.activation(z) \n",
    "\n",
    "    def update_loss(self, loss):\n",
    "        self.loss += loss\n",
    "        \n",
    "    def backward(self): \n",
    "        dl_dW = self.loss * self.grad * self.X  \n",
    "        dl_db =  self.loss * self.grad          \n",
    "        dl_dX = self.loss * self.grad * self.W \n",
    "        self.update_weights(dl_dW, dl_db)\n",
    "        return dl_dX\n",
    "    \n",
    "    def update_weights(self, dl_dW, dl_db):\n",
    "        self.W = self.W - self.alpha * dl_dW\n",
    "        self.b = self.b - self.alpha * dl_db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "16dde378-b753-4589-8f0d-8c4767bd46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, activation, derivative_activation, input_size, structure, learning_rate):\n",
    "        self.network = []\n",
    "        first_layer = [Neuron(activation, derivative_activation, learning_rate, input_size) for _ in range(structure[0])]\n",
    "        self.network.append(first_layer)\n",
    "        for i in range(1, len(structure)):\n",
    "            layer_i = [Neuron(activation, derivative_activation, learning_rate, structure[i - 1]) for _ in range(structure[i])]\n",
    "            self.network.append(layer_i)\n",
    "\n",
    "    def train(self, X_input, Y_label, iters):\n",
    "        final_loss = 0\n",
    "        epsilon = 0.00000001\n",
    "        n = len(self.network)\n",
    "        for it in range(iters):\n",
    "            for i, xinput in enumerate(X_input):\n",
    "                Y_hat = self.predict(xinput)\n",
    "                Y = Y_label[i]\n",
    "\n",
    "                del_loss = sum(2 * (Y - Y_hat) * (-1))\n",
    "                loss = [del_loss for i in range(len(self.network[-1]))]  # a vector with output layer's size\n",
    "                loss_value = sum(np.power((Y - Y_hat), 2))\n",
    "                if loss_value < epsilon / 1000 and abs(loss_value - prev_loss) < epsilon:\n",
    "                    final_loss = loss_value\n",
    "                    break\n",
    "                final_loss = loss_value\n",
    "\n",
    "                for layer in range(n-1, -1, -1):\n",
    "                    Parallel()(delayed(lambda neuron, unit_loss: neuron.update_loss(unit_loss))(self.network[layer][ni], loss[ni]) for ni in range(len(self.network[layer])))\n",
    "                    parallel_loss = Parallel()(delayed(lambda neuron: neuron.backward())(self.network[layer][ni]) for ni in range(len(self.network[layer])))\n",
    "                    loss = np.sum(parallel_loss, axis=0)  # loss to be propagated to the previous layer.\n",
    "        \n",
    "        print(f'Training concluded with a loss of {final_loss}.')\n",
    "\n",
    "    def predict(self, X):\n",
    "        nlen = len(self.network)\n",
    "        for layer in range(nlen):\n",
    "            a_in = Parallel()(delayed(lambda neuron: neuron.forward(X))(self.network[layer][ni]) for ni in range(len(self.network[layer])))\n",
    "            X = np.array(a_in)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fe20d10c-882b-401b-a3a0-8cda8d938373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training concluded with a loss of 4.783018253836828e-05.\n",
      "[0 0] [0.]\n",
      "[0 1] [1.]\n",
      "[1 1] [0.]\n",
      "[1 0] [1.]\n"
     ]
    }
   ],
   "source": [
    "network = NeuralNetwork(activation, derivative_activation, input_size=2, structure=[2,2,1], learning_rate=0.1)\n",
    "# 3 layered network, [*] \\-> [*] \\\n",
    "#                                 [*]\n",
    "#                    [*] /-> [*] / \n",
    "X_input = np.array([[0,0],[0,1],[1,1],[1,0]])\n",
    "Y_label = np.array([0,1,0,1])\n",
    "\n",
    "network.train(X_input, Y_label, 5000)\n",
    "for X in X_input:\n",
    "    print(X, np.round(abs(network.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b7a27-b9bd-4675-8b08-d12066792819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
