{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67dda7e2-153a-42e5-8fca-55ef9d35deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def activation(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def derivative_activation(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, activation, derivative_activation, alpha, num):\n",
    "        self.W = np.random.random(num) # num is the number of neurons in previous layer or input vector size\n",
    "        self.b = np.random.random()\n",
    "        self.alpha = alpha # learning rate\n",
    "        self.loss = 0\n",
    "        self.X = []\n",
    "        self.grad = 0\n",
    "        self.activation = activation \n",
    "        self.derivative_activation = derivative_activation \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Weights={self.W} Bias={self.b} loss={self.loss}'\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X # X is a vector of size num\n",
    "        self.loss = 0\n",
    "        self.dl_dw = np.zeros(len(X))\n",
    "        self.dl_db = 0\n",
    "        z = np.dot(self.W,self.X) + self.b \n",
    "        self.grad = self.derivative_activation(z)\n",
    "        return self.activation(z) \n",
    "\n",
    "    def update_loss(self, loss):\n",
    "        self.loss += loss\n",
    "        \n",
    "    def backward(self): \n",
    "        self.dl_dw = self.loss * self.grad * self.X  \n",
    "        self.dl_db =  self.loss * self.grad          \n",
    "        return self.loss * self.grad * self.W \n",
    "    \n",
    "    def update_weights(self):\n",
    "        self.W = self.W - self.alpha * self.dl_dw\n",
    "        self.b = self.b - self.alpha * self.dl_db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16dde378-b753-4589-8f0d-8c4767bd46db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss= 4.13952524789251e-05\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "layer1 = [Neuron(activation, derivative_activation, 0.1, 2) for i in range(2)]\n",
    "layer2 = [Neuron(activation, derivative_activation, 0.1, 2) for i in range(2)]\n",
    "layer3 = [Neuron(activation, derivative_activation, 0.1, 2)]\n",
    "network = [layer1, layer2, layer3]\n",
    "\n",
    "X_input = np.array([[0,0],[0,1],[1,1],[1,0]])\n",
    "Y_label = np.array([0,1,0,1])\n",
    "prev_loss = 0\n",
    "iters = 5000\n",
    "epsilon = 0.00000001\n",
    "    \n",
    "for it in range(iters):\n",
    "    \n",
    "    for i,xinput in enumerate(X_input):\n",
    "\n",
    "        nlen = len(network)\n",
    "        X = xinput\n",
    "        for layer in range(nlen):\n",
    "            a_in = Parallel()(delayed(lambda neuron: neuron.forward(X))(network[layer][i]) for i in range(len(network[layer])))\n",
    "            X = np.array(a_in)\n",
    "\n",
    "        Y = Y_label[i]\n",
    "      \n",
    "        del_loss = sum(2 * (Y-X) * (-1))   \n",
    "        loss = [del_loss for i in range(len(network[-1]))] # make this a vector with output layer's size\n",
    "        loss_value = sum(np.power((Y-X),2))\n",
    "        if loss_value < epsilon/1000 and abs(loss_value-prev_loss) < epsilon:\n",
    "            prev_loss = loss_value\n",
    "            break\n",
    "        prev_loss = loss_value\n",
    "\n",
    "        for layer in range(nlen-1,-1,-1):\n",
    "            Parallel()(delayed(lambda neuron, unit_loss: neuron.update_loss(unit_loss))(network[layer][i], loss[i]) for i in range(len(network[layer])))\n",
    "            loss_dim = len(network[layer-1]) if layer > 0 else len(X_input)\n",
    "            loss = np.zeros(loss_dim)\n",
    "            parallel_loss = Parallel()(delayed(lambda neuron: neuron.backward())(network[layer][i]) for i in range(len(network[layer])))\n",
    "            loss = np.sum(parallel_loss, axis=0)\n",
    "\n",
    "        Parallel()(delayed(lambda neuron: neuron.update_weights())(network[layer][i]) for layer in range(nlen) for i in range(len(network[layer])) )\n",
    "\n",
    "print('Final Loss=',prev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe20d10c-882b-401b-a3a0-8cda8d938373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    nlen = len(network)\n",
    "    for layer in range(nlen):\n",
    "        a_in = list()\n",
    "        for nrn in range(len(network[layer])):\n",
    "            val = network[layer][nrn].forward(X)\n",
    "            a_in.append(val)\n",
    "        X = np.array(a_in)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07ee80f2-bbaf-4e98-8170-78a43379a02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [0.]\n",
      "[0 1] [1.]\n",
      "[1 1] [0.]\n",
      "[1 0] [1.]\n"
     ]
    }
   ],
   "source": [
    "for X in X_input:\n",
    "    print(X, np.round(abs(predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b7a27-b9bd-4675-8b08-d12066792819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
